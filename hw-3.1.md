# Домашнее задание к занятию «Компоненты Kubernetes»

Исходя из условий задачи, приложение состоит из четырёх основных компонентов: базы данных, системы кеширования, backend-части и frontend-части. Так как приложение должно деплоиться в разные окружения, логично упаковать его в Helm chart с параметризацией количества реплик и ресурсов через values.yaml.

Расчёт ресурсов, необходимых каждому компоненту:

База данных должна быть отказоустойчивой, поэтому она разворачивается в трёх копиях. Каждая копия в рабочем состоянии потребляет 4 ГБ оперативной памяти и одно процессорное ядро. Следовательно, суммарно база данных требует 12 ГБ ОЗУ и 3 CPU.

Аналогичные требования предъявляются к системе кеширования: она также должна быть отказоустойчивой и состоять из трёх реплик. Каждая реплика потребляет 4 ГБ памяти и 1 CPU, значит на весь кеш требуется ещё 12 ГБ ОЗУ и 3 CPU.

Backend-часть приложения масштабируется горизонтально и разворачивается в десяти экземплярах. Один backend-под потребляет 600 МБ памяти и одно ядро процессора. В итоге backend в сумме требует около 6 ГБ оперативной памяти и 10 CPU.

Frontend обрабатывает внешние запросы и в основном отдаёт статику, поэтому его потребление ресурсов значительно меньше. Каждый экземпляр использует не более 50 МБ памяти и 0.2 CPU, а всего таких экземпляров пять. Следовательно, суммарно frontend потребляет примерно 250 МБ памяти и 1 CPU.

Складывая потребности всех компонентов, получаем, что приложение в целом требует около 17 процессорных ядер и примерно 30 ГБ оперативной памяти без учёта каких-либо запасов.

Так как кластер должен быть устойчив к отказу как минимум одной ноды, необходимо заложить резерв ресурсов. На практике обычно добавляют 20–30% запаса, чтобы кластер мог пережить отказ узла и при этом сохранить работоспособность всех критичных компонентов. В данном случае разумно заложить около 30%. Это увеличивает требования примерно до 24 CPU и 40 ГБ оперативной памяти.

Помимо ресурсов, потребляемых непосредственно приложением, необходимо учитывать служебные расходы Kubernetes. На каждой рабочей ноде часть ресурсов уходит на kubelet, container runtime, kube-proxy, CNI-плагины, а также возможные daemonset’ы для мониторинга и логирования. Обычно под эти нужды резервируют около 1 CPU и 2 ГБ памяти на ноду.

В качестве базовой конфигурации рабочих нод выберем универсальные ноды с 8 CPU и 16 ГБ оперативной памяти. После вычета служебных ресурсов каждая такая нода может предоставить под нагрузку примерно 7 CPU и 14 ГБ ОЗУ.

Если использовать четыре такие ноды, то суммарная доступная ёмкость составит 28 CPU и 56 ГБ памяти. Однако при выходе из строя одной ноды останется лишь 21 CPU и 42 ГБ памяти, что находится на грани рассчитанных требований. Чтобы не работать «впритык» и сохранить комфортный запас, целесообразно увеличить количество рабочих нод до пяти.

При пяти нодах кластер располагает 35 CPU и 70 ГБ оперативной памяти под пользовательские нагрузки. Даже при отказе одной ноды остаётся достаточно ресурсов для стабильной работы всех компонентов приложения.

Таким образом, итоговое решение — кластер из пяти рабочих нод с параметрами 8 CPU и 16 ГБ ОЗУ каждая, с учётом служебных ресурсов Kubernetes и запаса на отказ одной ноды. Такая конфигурация обеспечивает отказоустойчивость, масштабируемость и возможность дальнейшего роста нагрузки без немедленного пересмотра инфраструктуры.
